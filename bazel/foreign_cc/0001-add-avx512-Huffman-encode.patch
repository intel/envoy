From bc874c57fb355feadf2b0db042ec0d619e352457 Mon Sep 17 00:00:00 2001
From: Vladimir Medvedkin <vladimir.medvedkin@intel.com>
Date: Wed, 23 Feb 2022 16:30:44 +0000
Subject: [PATCH 1/2] add avx512 Huffman encode

to build additional flags are:
    CFLAGS='-g -O2 \
    -mavx512f -mavx512vl -mavx512cd -mavx512bw -mavx512dq \
    -mavx512vbmi -mavx512vbmi2 -mavx512bitalg' \
    ./configure --prefix=${PWD}/build-avx512

Signed-off-by: Vladimir Medvedkin <vladimir.medvedkin@intel.com>
---
 lib/nghttp2_hd_huffman.c        |   19 +
 lib/nghttp2_hd_huffman_avx512.h | 1086 +++++++++++++++++++++++++++++++
 2 files changed, 1105 insertions(+)
 create mode 100644 lib/nghttp2_hd_huffman_avx512.h

diff --git a/lib/nghttp2_hd_huffman.c b/lib/nghttp2_hd_huffman.c
index ac90f49c..d8f927bb 100644
--- a/lib/nghttp2_hd_huffman.c
+++ b/lib/nghttp2_hd_huffman.c
@@ -31,7 +31,14 @@
 #include "nghttp2_hd.h"
 #include "nghttp2_net.h"

+#include "nghttp2_hd_huffman_avx512.h"
+
 size_t nghttp2_hd_huff_encode_count(const uint8_t *src, size_t len) {
+
+#ifdef __AVX512F__
+	return nghttp2_hd_huff_encode_count_avx512(src, len);
+#endif
+
   size_t i;
   size_t nbits = 0;

@@ -54,6 +61,16 @@ int nghttp2_hd_huff_encode(nghttp2_bufs *bufs, const uint8_t *src,

   avail = nghttp2_bufs_cur_avail(bufs);

+#ifdef __AVX512F__
+
+  if (avail < srclen)
+    return NGHTTP2_ERR_NOMEM;
+
+  int enc_len = huff_encode_avx512(bufs->cur->buf.last, src, srclen);
+  bufs->cur->buf.last += enc_len;
+
+#else
+
   for (; src != end;) {
     sym = &huff_sym_table[*src++];
     code |= (uint64_t)sym->code << (32 - nbits);
@@ -100,6 +117,8 @@ int nghttp2_hd_huff_encode(nghttp2_bufs *bufs, const uint8_t *src,
     }
   }

+#endif
+
   return 0;
 }

diff --git a/lib/nghttp2_hd_huffman_avx512.h b/lib/nghttp2_hd_huffman_avx512.h
new file mode 100644
index 00000000..05686d0d
--- /dev/null
+++ b/lib/nghttp2_hd_huffman_avx512.h
@@ -0,0 +1,1086 @@
+#ifdef __AVX512F__
+
+#include <immintrin.h>
+
+static uint32_t codes_tbl[] = {
+	0xffc00000, 0xffffb000, 0xfffffe20, 0xfffffe30,
+	0xfffffe40, 0xfffffe50, 0xfffffe60, 0xfffffe70,
+	0xfffffe80, 0xffffea00, 0xfffffff0, 0xfffffe90,
+	0xfffffea0, 0xfffffff4, 0xfffffeb0, 0xfffffec0,
+	0xfffffed0, 0xfffffee0, 0xfffffef0, 0xffffff00,
+	0xffffff10, 0xffffff20, 0xfffffff8, 0xffffff30,
+	0xffffff40, 0xffffff50, 0xffffff60, 0xffffff70,
+	0xffffff80, 0xffffff90, 0xffffffa0, 0xffffffb0,
+	0x50000000, 0xfe000000, 0xfe400000, 0xffa00000,
+	0xffc80000, 0x54000000, 0xf8000000, 0xff400000,
+	0xfe800000, 0xfec00000, 0xf9000000, 0xff600000,
+	0xfa000000, 0x58000000, 0x5c000000, 0x60000000,
+	0x00000000, 0x08000000, 0x10000000, 0x64000000,
+	0x68000000, 0x6c000000, 0x70000000, 0x74000000,
+	0x78000000, 0x7c000000, 0xb8000000, 0xfb000000,
+	0xfff80000, 0x80000000, 0xffb00000, 0xff000000,
+	0xffd00000, 0x84000000, 0xba000000, 0xbc000000,
+	0xbe000000, 0xc0000000, 0xc2000000, 0xc4000000,
+	0xc6000000, 0xc8000000, 0xca000000, 0xcc000000,
+	0xce000000, 0xd0000000, 0xd2000000, 0xd4000000,
+	0xd6000000, 0xd8000000, 0xda000000, 0xdc000000,
+	0xde000000, 0xe0000000, 0xe2000000, 0xe4000000,
+	0xfc000000, 0xe6000000, 0xfd000000, 0xffd80000,
+	0xfffe0000, 0xffe00000, 0xfff00000, 0x88000000,
+	0xfffa0000, 0x18000000, 0x8c000000, 0x20000000,
+	0x90000000, 0x28000000, 0x94000000, 0x98000000,
+	0x9c000000, 0x30000000, 0xe8000000, 0xea000000,
+	0xa0000000, 0xa4000000, 0xa8000000, 0x38000000,
+	0xac000000, 0xec000000, 0xb0000000, 0x40000000,
+	0x48000000, 0xb4000000, 0xee000000, 0xf0000000,
+	0xf2000000, 0xf4000000, 0xf6000000, 0xfffc0000,
+	0xff800000, 0xfff40000, 0xffe80000, 0xffffffc0,
+	0xfffe6000, 0xffff4800, 0xfffe7000, 0xfffe8000,
+	0xffff4c00, 0xffff5000, 0xffff5400, 0xffffb200,
+	0xffff5800, 0xffffb400, 0xffffb600, 0xffffb800,
+	0xffffba00, 0xffffbc00, 0xffffeb00, 0xffffbe00,
+	0xffffec00, 0xffffed00, 0xffff5c00, 0xffffc000,
+	0xffffee00, 0xffffc200, 0xffffc400, 0xffffc600,
+	0xffffc800, 0xfffee000, 0xffff6000, 0xffffca00,
+	0xffff6400, 0xffffcc00, 0xffffce00, 0xffffef00,
+	0xffff6800, 0xfffee800, 0xfffe9000, 0xffff6c00,
+	0xffff7000, 0xffffd000, 0xffffd200, 0xfffef000,
+	0xffffd400, 0xffff7400, 0xffff7800, 0xfffff000,
+	0xfffef800, 0xffff7c00, 0xffffd600, 0xffffd800,
+	0xffff0000, 0xffff0800, 0xffff8000, 0xffff1000,
+	0xffffda00, 0xffff8400, 0xffffdc00, 0xffffde00,
+	0xfffea000, 0xffff8800, 0xffff8c00, 0xffff9000,
+	0xffffe000, 0xffff9400, 0xffff9800, 0xffffe200,
+	0xfffff800, 0xfffff840, 0xfffeb000, 0xfffe2000,
+	0xffff9c00, 0xffffe400, 0xffffa000, 0xfffff600,
+	0xfffff880, 0xfffff8c0, 0xfffff900, 0xfffffbc0,
+	0xfffffbe0, 0xfffff940, 0xfffff100, 0xfffff680,
+	0xfffe4000, 0xffff1800, 0xfffff980, 0xfffffc00,
+	0xfffffc20, 0xfffff9c0, 0xfffffc40, 0xfffff200,
+	0xffff2000, 0xffff2800, 0xfffffa00, 0xfffffa40,
+	0xffffffd0, 0xfffffc60, 0xfffffc80, 0xfffffca0,
+	0xfffec000, 0xfffff300, 0xfffed000, 0xffff3000,
+	0xffffa400, 0xffff3800, 0xffff4000, 0xffffe600,
+	0xffffa800, 0xffffac00, 0xfffff700, 0xfffff780,
+	0xfffff400, 0xfffff500, 0xfffffa80, 0xffffe800,
+	0xfffffac0, 0xfffffcc0, 0xfffffb00, 0xfffffb40,
+	0xfffffce0, 0xfffffd00, 0xfffffd20, 0xfffffd40,
+	0xfffffd60, 0xffffffe0, 0xfffffd80, 0xfffffda0,
+	0xfffffdc0, 0xfffffde0, 0xfffffe00, 0xfffffb80
+};
+
+static uint8_t code_len[] = {
+	13, 23, 28, 28, 28, 28, 28, 28, 28, 24, 30, 28, 28, 30, 28, 28,
+	28, 28, 28, 28, 28, 28, 30, 28, 28, 28, 28, 28, 28, 28, 28, 28,
+	6, 10, 10, 12, 13, 6, 8, 11, 10, 10, 8, 11, 8, 6, 6, 6,
+	5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 8, 15, 6, 12, 10,
+	13, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 13, 19, 13, 14, 6,
+	15, 5, 6, 5, 6, 5, 6, 6, 6, 5, 7, 7, 6, 6, 6, 5,
+	6, 7, 6, 5, 5, 6, 7, 7, 7, 7, 7, 15, 11, 14, 13, 28,
+	20, 22, 20, 20, 22, 22, 22, 23, 22, 23, 23, 23, 23, 23, 24, 23,
+	24, 24, 22, 23, 24, 23, 23, 23, 23, 21, 22, 23, 22, 23, 23, 24,
+	22, 21, 20, 22, 22, 23, 23, 21, 23, 22, 22, 24, 21, 22, 23, 23,
+	21, 21, 22, 21, 23, 22, 23, 23, 20, 22, 22, 22, 23, 22, 22, 23,
+	26, 26, 20, 19, 22, 23, 22, 25, 26, 26, 26, 27, 27, 26, 24, 25,
+	19, 21, 26, 27, 27, 26, 27, 24, 21, 21, 26, 26, 28, 27, 27, 27,
+	20, 24, 20, 21, 22, 21, 21, 23, 22, 22, 25, 25, 24, 24, 26, 23,
+	26, 27, 26, 26, 27, 27, 27, 27, 27, 28, 27, 27, 27, 27, 27, 26
+};
+
+static inline uint32_t
+add_reduce(__m512i acc) {
+	__m256i tmp256, acc256;
+	__m128i tmp128, acc128;
+	uint64_t acc64;
+
+	tmp256 = _mm512_extracti32x8_epi32(acc, 1);
+	acc256 = _mm512_castsi512_si256(acc);
+	acc256 = _mm256_add_epi32(acc256, tmp256);
+
+	tmp128 = _mm256_extracti32x4_epi32(acc256, 1);
+	acc128 =_mm256_castsi256_si128(acc256);
+	acc128 = _mm_add_epi32(acc128, tmp128);
+
+	tmp128 = _mm_shuffle_epi32(acc128, 0x4e);
+	acc128 = _mm_add_epi32(acc128, tmp128);
+
+	acc64 = _mm_extract_epi64(acc128, 0);
+
+	return (uint32_t)acc64 + (acc64 >> 32);
+}
+
+/*
+ * Calculates bit length of the codes. Up to 2^32 - 1 bits.
+ */
+static inline size_t
+nghttp2_hd_huff_encode_count_avx512(const uint8_t *src, size_t len)
+{
+	__m512i bytes, acc, acc_tmp, tmp1, tmp2;
+	__m512i lens, lens_1, lens_2, lens_3, lens_4;
+	__mmask64 k, l;
+	int i;
+	const __m512i perm_1b = _mm512_set_epi8(14, 15, 12, 13, 10, 11, 8, 9,
+						6, 7, 4, 5, 2, 3, 0, 1,
+						14, 15, 12, 13, 10, 11, 8, 9,
+						6, 7, 4, 5, 2, 3, 0, 1,
+						14, 15, 12, 13, 10, 11, 8, 9,
+						6, 7, 4, 5, 2, 3, 0, 1,
+						14, 15, 12, 13, 10, 11, 8, 9,
+						6, 7, 4, 5, 2, 3, 0, 1);
+
+	const __m512i perm_2b = _mm512_set_epi8 (13, 12, 15, 14, 9, 8, 11, 10,
+						5, 4, 7, 6, 1, 0, 3, 2,
+						13, 12, 15, 14, 9, 8, 11, 10,
+						5, 4, 7, 6, 1, 0, 3, 2,
+						13, 12, 15, 14, 9, 8, 11, 10,
+						5, 4, 7, 6, 1, 0, 3, 2,
+						13, 12, 15, 14, 9, 8, 11, 10,
+						5, 4, 7, 6, 1, 0, 3, 2);
+
+	acc = (__m512i)_mm512_setzero();
+	lens_1 = _mm512_load_epi64(code_len);
+	lens_2 = _mm512_load_epi64(code_len + 64);
+	lens_3 = _mm512_load_epi64(code_len + 128);
+	lens_4 = _mm512_load_epi64(code_len + 192);
+
+
+	while (len >= 64) {
+		acc_tmp = (__m512i)_mm512_setzero();
+		for (i = 0; (i < 8) && (len >= 64); i++) {
+			bytes = _mm512_loadu_si512(src);
+			src += 64;
+			len -= 64;
+			/* get corresponding code lengths */
+			tmp1 = _mm512_permutex2var_epi8(lens_1, bytes, lens_2);
+			tmp2 = _mm512_permutex2var_epi8(lens_3, bytes, lens_4);
+			k = _mm512_movepi8_mask(bytes);
+			lens = _mm512_mask_blend_epi8(k, tmp1, tmp2);
+			/*
+			 * accumulate bit lens.
+			 * can safely add up to 8 times due to maximum length = 30
+			 */
+			acc_tmp = _mm512_add_epi8(acc_tmp, lens);
+		}
+		tmp1 = _mm512_maskz_mov_epi8(0x5555555555555555, acc_tmp);
+		tmp2 = _mm512_maskz_shuffle_epi8(0x5555555555555555, acc_tmp, perm_1b);
+		tmp1 = _mm512_add_epi16(tmp1, tmp2);
+
+		tmp2 = _mm512_maskz_mov_epi8(0x3333333333333333, tmp1);
+		tmp1 = _mm512_maskz_shuffle_epi8(0x3333333333333333, tmp1, perm_2b);
+		tmp2 = _mm512_add_epi32(tmp1, tmp2);
+
+		acc = _mm512_add_epi32(acc, tmp2);
+	}
+
+	l = (1ULL << len) - 1;
+	bytes = _mm512_maskz_loadu_epi8(l, src);
+	tmp1 = _mm512_maskz_permutex2var_epi8(l, lens_1, bytes, lens_2);
+	tmp2 = _mm512_maskz_permutex2var_epi8(l, lens_3, bytes, lens_4);
+	k = _mm512_movepi8_mask(bytes);
+	lens = _mm512_mask_blend_epi8(k, tmp1, tmp2);
+
+	tmp1 = _mm512_maskz_mov_epi8(0x5555555555555555, lens);
+	tmp2 = _mm512_maskz_shuffle_epi8(0x5555555555555555, lens, perm_1b);
+	tmp1 = _mm512_add_epi16(tmp1, tmp2);
+
+	tmp2 = _mm512_maskz_mov_epi8(0x3333333333333333, tmp1);
+	tmp1 = _mm512_maskz_shuffle_epi8(0x3333333333333333, tmp1, perm_2b);
+	tmp2 = _mm512_add_epi32(tmp1, tmp2);
+
+	acc = _mm512_add_epi32(acc, tmp2);
+
+	return (add_reduce(acc) + 7) / 8;
+}
+
+
+/**
+ * Shifts bits in 128bit lane of @a
+ * Shifts register contains shift values per lane
+ * in first byte of the lane
+ */
+static inline __m512i
+shift_l_128_lane(__m512i a, __m512i shifts)
+{
+	__m512i tmp;
+
+	/* copy 1 <- 3, 2 <- 4 per lane */
+	tmp = _mm512_maskz_shuffle_epi32(0xcccc, a, 0x44);
+	tmp = _mm512_mask_shldv_epi64(a, 0xaa, tmp, shifts);
+	return _mm512_mask_sllv_epi64(tmp, 0x55, a, shifts);
+}
+
+static inline __m512i
+shift_r_128_lane(__m512i a, __m512i shifts)
+{
+	__m512i tmp;
+
+	/* copy 3 <- 1, 4 <- 2 per lane */
+	tmp = _mm512_maskz_shuffle_epi32(0x3333, a, 0xee);
+	tmp = _mm512_mask_shrdv_epi64(a, 0x55, tmp, shifts);
+	return _mm512_mask_srlv_epi64(tmp, 0xaa, a, shifts);
+}
+
+/**
+ * merge codes from @codes with respecting length@
+ * adds remainig codes as the most right bits
+ * write back to @buf_out completed bytes from @codes bits's
+ *
+ * returns remaining non written bits (shifted right) and
+ * remaining length (in bits).
+ *
+ */
+static inline __m512i
+compress_codes(__m512i codes, __m512i lens, __m512i rem_codes,
+	__m512i *rem_len, uint8_t *rem_len_scalar,
+	uint8_t *total_len_ret, uint8_t *buf_out)
+{
+	const __m512i thirty_two = _mm512_set1_epi32(32);
+	const __m512i sixty_four = _mm512_set1_epi64(64);
+	__m512i shifts;
+	__m512i tmp;
+
+	/**
+	 * shift right every even 32bit element of codes by
+	 * (32 - len[n]) bits to merge with code an odd element
+	 * and shift the whole back as a 64bit element
+	 */
+	shifts = _mm512_maskz_sub_epi32(0xaaaa, thirty_two, lens);
+	codes = _mm512_srlv_epi32(codes, shifts);
+	shifts = _mm512_maskz_shuffle_epi32(0x5555, shifts, 0x31);
+	codes =  _mm512_sllv_epi64(codes, shifts);
+
+	/**
+	 * Update lens.
+	 * sum 2 adjacent 32bit values of lens
+	 */
+	/* copy 1 <- 2, 3 <- 4 per lane */
+	tmp = _mm512_maskz_shuffle_epi32(0x5555, lens, 0xf5);
+	lens = _mm512_maskz_add_epi32(0x5555, lens, tmp);
+
+	/**
+	 * shift right every even 64bit element of codes by
+	 * (64 - len[n]) bits to merge with codes of odd elements
+	 * and shift the whole lane back as a 128bit element
+	 */
+	shifts = _mm512_maskz_sub_epi64(0xaa, sixty_four, lens);
+	codes = _mm512_srlv_epi64(codes, shifts);
+	/* copy 1 <- 3, 2 <- 4 per lane */
+	shifts = _mm512_shuffle_epi32(shifts, 0xee);
+	codes = shift_l_128_lane(codes, shifts);
+
+	/**
+	 * sum 2 adjacent 64bit values of lens
+	 */
+	/* copy 1 <- 3, 2 <- 4 per lane */
+	tmp = _mm512_maskz_shuffle_epi32(0x3333, lens, 0xee);
+	lens = _mm512_maskz_add_epi64(0x55, lens, tmp); // SUM(Ln,Ln+3)
+
+	/**
+	 * Merge previous remaining bits.
+	 * rem_len must have remaining length in 6th and 7th epi64 of the zmm.
+	 */
+	codes = shift_r_128_lane(codes, *rem_len);
+	codes = _mm512_or_epi64(codes, rem_codes);
+	lens = _mm512_mask_add_epi64(lens, 0x1 << 6, lens, *rem_len);
+
+	/**
+	 * get partial offsets (in bits) for every code in 128bit lane
+	 */
+	const __m512i perm_len_1 = _mm512_set_epi64(0, 0, 6, 6, 4, 4, 2, 2);
+	const __m512i perm_len_2 = _mm512_set_epi64(0, 0, 0, 0, 0, 6, 0, 4);
+	__m512i part_offset;
+
+	/* tmp has values from lens shifted 1 128bit lane left */
+	tmp = _mm512_maskz_permutexvar_epi64(0x15, perm_len_1, lens);
+	/* tmp has S(1-4), S(1-8), S(5-12), S(9-16). S(1-4) is on MSB side */
+	tmp = _mm512_add_epi64(lens, tmp);
+	/* part_offset has values from the previous sum result
+	 * shifted left by 2 128bit lanes
+	*/
+	part_offset = _mm512_maskz_permutexvar_epi64(0x5, perm_len_2, tmp);
+	/* now part_offset have offsets in bits,
+	* i.e S(1-4), S(1-8), S(1-12), S(1-16)
+	*/
+	part_offset = _mm512_add_epi64(tmp, part_offset);
+
+	// maybe do shift by 3 here? I.e. 0x1009080706050403
+	uint64_t lens_compressed;
+	const __m512i low_bits = _mm512_set1_epi64(0x0706050403020100);
+	const __m512i high_bits = _mm512_set1_epi64(0x0f0e0d0c0b0a0908);
+	lens_compressed = _mm512_bitshuffle_epi64_mask(part_offset, low_bits);
+	lens_compressed |= (_mm512_bitshuffle_epi64_mask(part_offset, high_bits) << 8);
+
+	/**
+	 * Gather codes in 4 128bit lanes into consecutive bitstream
+	 */
+	__m512i seq_1 = _mm512_set1_epi8(16);
+	__m512i seq_2 = _mm512_set1_epi8(32);
+	__m512i seq_3 = _mm512_set1_epi8(48);
+	const __m512i seq = _mm512_set_epi8(63, 62, 61, 60, 59, 58, 57, 56,
+					55, 54, 53, 52, 51, 50, 49, 48,
+					47, 46, 45, 44, 43, 42, 41, 40,
+					39, 38, 37, 36, 35, 34, 33, 32,
+					31, 30, 29, 28, 27, 26, 25, 24,
+					23, 22, 21, 20, 19, 18, 17, 16,
+					15, 14, 13, 12, 11, 10, 9, 8,
+					7, 6, 5, 4, 3, 2, 1, 0);
+
+	/* now tmp have offsets in bytes */
+	tmp = _mm512_srli_epi64(part_offset, 3);
+
+	__m512i perm_idx_1, perm_idx_2, perm_idx_3;
+	perm_idx_1 = _mm512_permutexvar_epi8(seq_3, tmp);
+	perm_idx_2 = _mm512_permutexvar_epi8(seq_2, tmp);
+	perm_idx_3 = _mm512_permutexvar_epi8(seq_1, tmp);
+
+	seq_1 = _mm512_sub_epi8(seq, seq_1);
+	seq_2 = _mm512_sub_epi8(seq, seq_2);
+	seq_3 = _mm512_sub_epi8(seq, seq_3);
+
+	perm_idx_1 =  _mm512_add_epi8(seq_1, perm_idx_1);
+	perm_idx_2 =  _mm512_add_epi8(seq_2, perm_idx_2);
+	perm_idx_3 =  _mm512_add_epi8(seq_3, perm_idx_3);
+
+	__m512i tmp_res_1, tmp_res_2, tmp_res_3;
+	uint64_t add_mask = 0xFF;
+
+	tmp = _mm512_maskz_set1_epi64(0x55, 0x7);
+	shifts = _mm512_and_epi64(tmp, part_offset);
+	shifts = _mm512_maskz_permutexvar_epi64(0x3f, perm_len_1, shifts);
+	codes = shift_r_128_lane(codes, shifts);
+
+	add_mask = (uint64_t)(0xffff) << (uint8_t)(48 - (lens_compressed >> (3 + 48)));
+	tmp_res_1 = _mm512_maskz_permutexvar_epi8(add_mask, perm_idx_1, codes);
+
+	add_mask = (uint64_t)(0xffff) << (uint8_t)(48 - (lens_compressed >> (3 + 32)));
+	tmp_res_2 = _mm512_maskz_permutexvar_epi8(add_mask, perm_idx_2, codes);
+
+	add_mask = (uint64_t)(0xffff) << (uint8_t)(48 - (lens_compressed >> (3 + 16)));
+	tmp_res_3 = _mm512_maskz_permutexvar_epi8(add_mask, perm_idx_3, codes);
+
+	__m512i ret;
+	ret = _mm512_maskz_mov_epi64(0xc0, codes);
+	ret = _mm512_or_epi64(ret, tmp_res_1);
+	ret = _mm512_or_epi64(ret, tmp_res_2);
+	ret = _mm512_or_epi64(ret, tmp_res_3);
+
+	uint8_t total_len = (uint8_t)(lens_compressed >> 3);
+	add_mask = (1ULL << total_len) - 1;
+	__m512i rev_order = _mm512_set_epi8(0, 1, 2, 3, 4, 5, 6, 7,
+					8, 9, 10, 11, 12, 13, 14, 15,
+					16, 17, 18, 19, 20, 21, 22, 23,
+					24, 25, 26, 27, 28, 29, 30, 31,
+					32, 33, 34, 35, 36, 37, 38, 39,
+					40, 41, 42, 43, 44, 45, 46, 47,
+					48, 49, 50, 51, 52, 53, 54, 55,
+					56, 57, 58, 59, 60, 61, 62, 63);
+
+	*rem_len = _mm512_maskz_set1_epi8((1ULL << 56)|(1ULL << (56 - 8)), lens_compressed & 0x7);
+	*rem_len_scalar = lens_compressed & 0x7;
+
+	/*
+	 * copy last incomplete bits of ret to the MSB
+	 * to merge them in the following invocation
+	 */
+	tmp = _mm512_maskz_set1_epi8((1ULL << 63), (63 - total_len));
+	rev_order = _mm512_add_epi8(rev_order, tmp);
+	ret = _mm512_permutexvar_epi8(rev_order, ret);
+	_mm512_mask_storeu_epi8(buf_out, add_mask, ret);
+	ret = _mm512_maskz_mov_epi8((1ULL << 63), ret);
+
+	*total_len_ret = total_len;
+	return ret;
+}
+
+#define REV_ORDER	0, 1, 2, 3, 4, 5, 6, 7,		\
+			8, 9, 10, 11, 12, 13, 14, 15,	\
+			16, 17, 18, 19, 20, 21, 22, 23,	\
+			24, 25, 26, 27, 28, 29, 30, 31,	\
+			32, 33, 34, 35, 36, 37, 38, 39,	\
+			40, 41, 42, 43, 44, 45, 46, 47,	\
+			48, 49, 50, 51, 52, 53, 54, 55,	\
+			56, 57, 58, 59, 60, 61, 62, 63
+
+#define PERM_1B		14, 15, 12, 13, 10, 11, 8, 9,	\
+			6, 7, 4, 5, 2, 3, 0, 1,		\
+			14, 15, 12, 13, 10, 11, 8, 9,	\
+			6, 7, 4, 5, 2, 3, 0, 1,		\
+			14, 15, 12, 13, 10, 11, 8, 9,	\
+			6, 7, 4, 5, 2, 3, 0, 1,		\
+			14, 15, 12, 13, 10, 11, 8, 9,	\
+			6, 7, 4, 5, 2, 3, 0, 1
+
+#define PERM_2B		13, 12, 15, 14, 9, 8, 11, 10,	\
+			5, 4, 7, 6, 1, 0, 3, 2,		\
+			13, 12, 15, 14, 9, 8, 11, 10,	\
+			5, 4, 7, 6, 1, 0, 3, 2,		\
+			13, 12, 15, 14, 9, 8, 11, 10,	\
+			5, 4, 7, 6, 1, 0, 3, 2,		\
+			13, 12, 15, 14, 9, 8, 11, 10,	\
+			5, 4, 7, 6, 1, 0, 3, 2
+
+#define SEQ		0, 0, 0, 0, 1, 1, 1, 1,		\
+			2, 2, 2, 2, 3, 3, 3, 3,		\
+			4, 4, 4, 4, 5, 5, 5, 5,		\
+			6, 6, 6, 6, 7, 7, 7, 7,		\
+			8, 8, 8, 8, 9, 9, 9, 9,		\
+			10, 10, 10, 10, 11, 11, 11, 11,	\
+			12, 12, 12, 12, 13, 13, 13, 13,	\
+			14, 14, 14, 14, 15, 15, 15, 15
+
+#define CODES_FIRST_00  255, 255, 128, 255, 251, 184, 124, 120, \
+                        116, 112, 108, 104, 100, 16, 8, 0,      \
+                        96, 92, 88, 250, 255, 249, 254, 254,    \
+                        255, 248, 84, 255, 255, 254, 254, 80,   \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255
+
+#define CODES_FIRST_01  255, 255, 255, 255, 255, 246, 244, 242, \
+                        240, 238, 180, 72, 64, 176, 236, 172,   \
+                        56, 168, 164, 160, 234, 232, 48, 156,   \
+                        152, 148, 40, 144, 32, 140, 24, 255,    \
+                        136, 255, 255, 255, 255, 253, 230, 252, \
+                        228, 226, 224, 222, 220, 218, 216, 214, \
+                        212, 210, 208, 206, 204, 202, 200, 198, \
+                        196, 194, 192, 190, 188, 186, 132, 255
+
+#define CODES_FIRST_10  255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255
+
+
+#define CODES_FIRST_11  255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255
+
+#define CODES_SECOND_00 0, 176, 0, 248, 0, 0, 0, 0,             \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 96, 0, 192, 128,            \
+                        64, 0, 0, 200, 160, 64, 0, 0,           \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 192
+
+#define CODES_SECOND_01 255, 232, 244, 128, 252, 0, 0, 0,       \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 250,               \
+                        0, 240, 224, 254, 216, 0, 0, 0,         \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 208
+
+#define CODES_SECOND_10 255, 255, 255, 255, 255, 255, 255, 254, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 254, 255, 255, 255, 255, \
+                        254, 255, 255, 255, 255, 254, 254, 255, \
+                        255, 255, 255, 255, 255, 255, 254, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 254, 254, 255, 254
+
+#define CODES_SECOND_11 255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 254, 255, 254, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 255, 255, 254, \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 254, 254, 255, 255
+
+#define CODES_THIRD_00  0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        255, 255, 255, 255, 255, 255, 255, 255, \
+                        255, 255, 255, 255, 255, 254, 254, 254, \
+                        254, 254, 255, 254, 254, 255, 234, 254, \
+                        254, 254, 254, 254, 254, 254, 176, 0
+
+#define CODES_THIRD_01  255, 0, 0, 0, 0, 0, 0, 0,               \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0
+
+#define CODES_THIRD_10  226, 152, 148, 224, 144, 140, 136, 160, \
+                        222, 220, 132, 218, 16, 128, 8, 0,      \
+                        216, 214, 124, 248, 240, 120, 116, 212, \
+                        240, 210, 208, 112, 108, 144, 232, 104, \
+                        239, 206, 204, 100, 202, 96, 224, 200,  \
+                        198, 196, 194, 238, 192, 92, 237, 236,  \
+                        190, 235, 188, 186, 184, 182, 180, 88,  \
+                        178, 84, 80, 76, 128, 112, 72, 96
+
+#define CODES_THIRD_11  251, 254, 253, 253, 253, 253, 255, 253, \
+                        253, 253, 253, 252, 251, 251, 252, 250, \
+                        232, 250, 245, 244, 247, 247, 172, 168, \
+                        230, 64, 56, 164, 48, 208, 243, 192,    \
+                        252, 252, 252, 255, 250, 250, 40, 32,   \
+                        242, 252, 249, 252, 252, 249, 24, 64,   \
+                        246, 241, 249, 251, 251, 249, 248, 248, \
+                        246, 160, 228, 156, 32, 176, 248, 248
+
+#define CODES_FOURTH_00 0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        176, 160, 144, 128, 112, 96, 80, 64,    \
+                        48, 248, 32, 16, 0, 240, 224, 208,      \
+                        192, 176, 244, 160, 144, 240, 0, 128,   \
+                        112, 96, 80, 64, 48, 32, 0, 0
+
+#define CODES_FOURTH_01 192, 0, 0, 0, 0, 0, 0, 0,               \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0
+
+#define CODES_FOURTH_10 0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        0, 0, 0, 0, 0, 0, 0, 0
+
+#define CODES_FOURTH_11 128, 0, 224, 192, 160, 128, 224, 96,    \
+                        64, 32, 0, 224, 64, 0, 192, 192,        \
+                        0, 128, 0, 0, 128, 0, 0, 0,             \
+                        0, 0, 0, 0, 0, 0, 0, 0,                 \
+                        160, 128, 96, 208, 64, 0, 0, 0,         \
+                        0, 64, 192, 32, 0, 128, 0, 0,           \
+                        128, 0, 64, 224, 192, 0, 192, 128,      \
+                        0, 0, 0, 0, 0, 0, 64, 0
+
+static inline __m512i
+nghttp2_mm512_srlv_epi8(__m512i codes, __m512i shifts)
+{
+	__m512i tmp;
+	const __m512i perm_1b = _mm512_set_epi8(PERM_1B);
+
+        shifts = _mm512_maskz_shuffle_epi8(0x5555555555555555, shifts, perm_1b);
+        tmp = _mm512_maskz_mov_epi8(0xaaaaaaaaaaaaaaaa, codes);
+        tmp = _mm512_srlv_epi16(tmp, shifts);
+
+        return _mm512_mask_mov_epi8(tmp, 0x5555555555555555, codes);
+}
+
+//#define USE_LOOP
+#define USE_GATHER
+//#define OPT_A
+static inline void
+load_codes(__m512i bytes, __m512i *a, __m512i *b, __m512i *c, __m512i *d)
+{
+#ifndef USE_GATHER
+        __m512i codes_first_0, codes_first_1, codes_first_2, codes_first_3;
+        __m512i codes_second_0, codes_second_1, codes_second_2, codes_second_3;
+        __m512i codes_third_0, codes_third_1, codes_third_2, codes_third_3;
+        __m512i codes_fourth_0, codes_fourth_1, codes_fourth_2, codes_fourth_3;
+        __m512i codes_chunks_0, codes_chunks_1, codes_chunks_2, codes_chunks_3;
+        __m512i tmp_1, tmp_2, tmp_3, tmp_4;
+        __m512i seq = _mm512_set_epi8(SEQ);
+        const __m512i sixteen = _mm512_set1_epi8(16);
+        __mmask64 k;
+
+        codes_first_0 = _mm512_set_epi8(CODES_FIRST_00);
+        codes_first_1 = _mm512_set_epi8(CODES_FIRST_01);
+        codes_first_2 = _mm512_set_epi8(CODES_FIRST_10);
+        codes_first_3 = _mm512_set_epi8(CODES_FIRST_11);
+
+        codes_second_0 = _mm512_set_epi8(CODES_SECOND_00);
+        codes_second_1 = _mm512_set_epi8(CODES_SECOND_01);
+        codes_second_2 = _mm512_set_epi8(CODES_SECOND_10);
+        codes_second_3 = _mm512_set_epi8(CODES_SECOND_11);
+
+        codes_third_0 = _mm512_set_epi8(CODES_THIRD_00);
+        codes_third_1 = _mm512_set_epi8(CODES_THIRD_01);
+        codes_third_2 = _mm512_set_epi8(CODES_THIRD_10);
+        codes_third_3 = _mm512_set_epi8(CODES_THIRD_11);
+
+        codes_fourth_0 = _mm512_set_epi8(CODES_FOURTH_00);
+        codes_fourth_1 = _mm512_set_epi8(CODES_FOURTH_01);
+        codes_fourth_2 = _mm512_set_epi8(CODES_FOURTH_10);
+        codes_fourth_3 = _mm512_set_epi8(CODES_FOURTH_11);
+
+        k = _mm512_movepi8_mask(bytes);
+
+        tmp_1 = _mm512_permutex2var_epi8(codes_first_0, bytes, codes_first_1);
+        tmp_2 = _mm512_permutex2var_epi8(codes_first_2, bytes, codes_first_3);
+        tmp_3 = _mm512_permutex2var_epi8(codes_second_0, bytes, codes_second_1);
+        tmp_4 = _mm512_permutex2var_epi8(codes_second_2, bytes, codes_second_3);
+        codes_chunks_0 = _mm512_mask_blend_epi8(k, tmp_1, tmp_2); // <- MSB
+        codes_chunks_1 = _mm512_mask_blend_epi8(k, tmp_3, tmp_4);
+        tmp_1 = _mm512_permutex2var_epi8(codes_third_0, bytes, codes_third_1);
+        tmp_2 = _mm512_permutex2var_epi8(codes_third_2, bytes, codes_third_3);
+        tmp_3 = _mm512_permutex2var_epi8(codes_fourth_0, bytes, codes_fourth_1);
+        tmp_4 = _mm512_permutex2var_epi8(codes_fourth_2, bytes, codes_fourth_3);
+        codes_chunks_2 = _mm512_mask_blend_epi8(k, tmp_1, tmp_2);
+        codes_chunks_3 = _mm512_mask_blend_epi8(k, tmp_3, tmp_4); // <- LSB
+
+#ifdef OPT_A
+        tmp_1 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_2 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_3 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_4 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_mask_permutexvar_epi8(tmp_1, 0x4444444444444444, seq, codes_chunks_1);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_2 = _mm512_mask_permutexvar_epi8(tmp_2, 0x4444444444444444, seq, codes_chunks_1);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_3 = _mm512_mask_permutexvar_epi8(tmp_3, 0x4444444444444444, seq, codes_chunks_1);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_4 = _mm512_mask_permutexvar_epi8(tmp_4, 0x4444444444444444, seq, codes_chunks_1);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_mask_permutexvar_epi8(tmp_1, 0x2222222222222222, seq, codes_chunks_2);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_2 = _mm512_mask_permutexvar_epi8(tmp_2, 0x2222222222222222, seq, codes_chunks_2);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_3 = _mm512_mask_permutexvar_epi8(tmp_3, 0x2222222222222222, seq, codes_chunks_2);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_4 = _mm512_mask_permutexvar_epi8(tmp_4, 0x2222222222222222, seq, codes_chunks_2);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_mask_permutexvar_epi8(tmp_1, 0x1111111111111111, seq, codes_chunks_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_2 = _mm512_mask_permutexvar_epi8(tmp_2, 0x1111111111111111, seq, codes_chunks_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_3 = _mm512_mask_permutexvar_epi8(tmp_3, 0x1111111111111111, seq, codes_chunks_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+        tmp_4 = _mm512_mask_permutexvar_epi8(tmp_4, 0x1111111111111111, seq, codes_chunks_3);
+
+        *a = tmp_1;
+        *b = tmp_2;
+        *c = tmp_3;
+        *d = tmp_4;
+#else
+        tmp_1 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        tmp_2 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, seq, codes_chunks_1);
+        tmp_3 = _mm512_maskz_permutexvar_epi8(0x2222222222222222, seq, codes_chunks_2);
+        tmp_4 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, codes_chunks_3);
+        tmp_1 = _mm512_or_si512(tmp_1, tmp_2);
+        tmp_3 = _mm512_or_si512(tmp_3, tmp_4);
+        *a = _mm512_or_si512(tmp_1, tmp_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        tmp_2 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, seq, codes_chunks_1);
+        tmp_3 = _mm512_maskz_permutexvar_epi8(0x2222222222222222, seq, codes_chunks_2);
+        tmp_4 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, codes_chunks_3);
+        tmp_1 = _mm512_or_si512(tmp_1, tmp_2);
+        tmp_3 = _mm512_or_si512(tmp_3, tmp_4);
+        *b = _mm512_or_si512(tmp_1, tmp_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        tmp_2 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, seq, codes_chunks_1);
+        tmp_3 = _mm512_maskz_permutexvar_epi8(0x2222222222222222, seq, codes_chunks_2);
+        tmp_4 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, codes_chunks_3);
+        tmp_1 = _mm512_or_si512(tmp_1, tmp_2);
+        tmp_3 = _mm512_or_si512(tmp_3, tmp_4);
+        *c = _mm512_or_si512(tmp_1, tmp_3);
+        seq = _mm512_add_epi8(seq, sixteen);
+
+        tmp_1 = _mm512_maskz_permutexvar_epi8(0x8888888888888888, seq, codes_chunks_0);
+        tmp_2 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, seq, codes_chunks_1);
+        tmp_3 = _mm512_maskz_permutexvar_epi8(0x2222222222222222, seq, codes_chunks_2);
+        tmp_4 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, codes_chunks_3);
+        tmp_1 = _mm512_or_si512(tmp_1, tmp_2);
+        tmp_3 = _mm512_or_si512(tmp_3, tmp_4);
+        *d = _mm512_or_si512(tmp_1, tmp_3);
+#endif
+
+#else
+        __m512i idxes_1, idxes_2, idxes_3, idxes_4;
+        __m512i seq = _mm512_set_epi32(0, 1, 2, 3,
+                                        4, 5, 6, 7,
+                                        8, 9, 10, 11,
+                                        12, 13, 14, 15);
+        const __m512i sixteen = _mm512_set1_epi32(16);
+        idxes_1 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+        seq = _mm512_add_epi8(seq, sixteen);
+        idxes_2 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+        seq = _mm512_add_epi8(seq, sixteen);
+        idxes_3 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+        seq = _mm512_add_epi8(seq, sixteen);
+        idxes_4 = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+        seq = _mm512_add_epi8(seq, sixteen);
+        *a = _mm512_i32gather_epi32(idxes_1, codes_tbl, 4);
+        *b = _mm512_i32gather_epi32(idxes_2, codes_tbl, 4);
+        *c = _mm512_i32gather_epi32(idxes_3, codes_tbl, 4);
+        *d = _mm512_i32gather_epi32(idxes_4, codes_tbl, 4);
+        return;
+#endif
+}
+
+/**
+ * Try to do an optimization.
+ * Depending on the codes length completes one or two
+ * sets of registers with merged codes/lens.
+ * Returns:
+ *  1 - if input codes have length less then 8 bits
+ *  2 - if input codes have length less then 16 bits
+ *  0 - otherwise
+ * Note: compiler should optimize code using use_mask
+ */
+static inline int
+merge_shorter_codes(__m512i bytes, __m512i lens_64, int len,
+        __m512i *codes_one, __m512i *codes_two,
+        __m512i *lens_one, __m512i *lens_two, int use_mask)
+{
+        const __m512i rev_order = _mm512_set_epi8(REV_ORDER);
+        const __m512i perm_1b = _mm512_set_epi8(PERM_1B);
+        const __m512i perm_2b = _mm512_set_epi8(PERM_2B);
+        const __m512i codes_hi_one = _mm512_set_epi8(CODES_FIRST_00);
+        const __m512i codes_hi_two = _mm512_set_epi8(CODES_FIRST_01);
+        const __m512i codes_low_one = _mm512_set_epi8(CODES_SECOND_00);
+        const __m512i codes_low_two = _mm512_set_epi8(CODES_SECOND_01);
+        const __m512i eight = _mm512_set1_epi8(8);
+        const __m512i sixteen = _mm512_set1_epi8(16);
+        const __m512i thirty_two = _mm512_set1_epi8(32);
+        __m512i tmp;
+        __mmask64 l_mask_8;
+        __mmask64 l_mask_16;
+        __m512i shifts, shifts_back;
+        __m512i codes;
+        __m512i lens_32;
+	__mmask64 mask_64;
+
+        l_mask_8 = _mm512_cmp_epi8_mask(lens_64, eight, 1);
+        l_mask_16 = _mm512_cmp_epi8_mask(lens_64, sixteen, 1);
+
+        if (l_mask_8 == UINT64_MAX) {
+                lens_64 = _mm512_permutexvar_epi8(rev_order, lens_64);
+                bytes = _mm512_permutexvar_epi8(rev_order, bytes);
+
+                shifts = _mm512_maskz_sub_epi8(0xaaaaaaaaaaaaaaaa, eight, lens_64);
+                shifts_back = _mm512_shuffle_epi8(shifts, perm_1b);
+		if (use_mask) {
+			mask_64 = UINT64_MAX << (64 - len);
+	                codes = _mm512_maskz_permutex2var_epi8(mask_64, codes_hi_one, bytes, codes_hi_two);
+		} else
+	                codes = _mm512_permutex2var_epi8(codes_hi_one, bytes, codes_hi_two);
+
+		codes = nghttp2_mm512_srlv_epi8(codes, shifts);
+		codes = _mm512_sllv_epi16(codes, shifts_back);
+
+                tmp = _mm512_maskz_shuffle_epi8(0x5555555555555555, lens_64, perm_1b);
+                lens_32 = _mm512_maskz_add_epi8(0x5555555555555555, lens_64, tmp);
+
+                tmp = _mm512_maskz_mov_epi8(0x5555555555555555, sixteen);
+                shifts = _mm512_maskz_sub_epi16(0xaaaaaaaa, tmp, lens_32);
+                shifts_back =  _mm512_shuffle_epi8(shifts, perm_2b);
+                codes = _mm512_srlv_epi16(codes, shifts);
+                *codes_one = _mm512_sllv_epi32(codes, shifts_back);
+
+                tmp = _mm512_maskz_shuffle_epi8(0x3333333333333333, lens_32, perm_2b);
+                *lens_one = _mm512_maskz_add_epi16(0x55555555, lens_32, tmp);
+
+                return 1;
+
+        } else if (l_mask_16 == UINT64_MAX) {
+                __m512i a,b,c,d;
+		__mmask64 mask_64_1, mask_64_2;
+                const __m512i perm_2_var_idxes = _mm512_set_epi8(0, 64, 1, 65, 2, 66, 3, 67,
+                                                4, 68, 5, 69, 6, 70, 7, 71,
+                                                8, 72, 9, 73, 10, 74, 11, 75,
+                                                12, 76, 13, 77, 14, 78, 15, 79,
+                                                16, 80, 17, 81, 18, 82, 19, 83,
+                                                20, 84, 21, 85, 22, 86, 23, 87,
+                                                24, 88, 25, 89, 26, 90, 27, 91,
+                                                28, 92, 29, 93, 30, 94, 31, 95);
+                const __m512i permute_lens = _mm512_set_epi32(0, 2, 4, 6,
+                                                8, 10, 12, 14,
+                                                16, 18, 20, 22,
+                                                24, 26, 28, 30);
+
+                a = _mm512_permutex2var_epi8(codes_hi_one, bytes, codes_hi_two);
+                b = _mm512_permutex2var_epi8(codes_low_one, bytes, codes_low_two);
+
+                tmp = _mm512_maskz_shuffle_epi8(0x5555555555555555, lens_64, perm_1b);
+                lens_32 = _mm512_maskz_add_epi8(0x5555555555555555, lens_64, tmp);
+
+                tmp = _mm512_add_epi8(thirty_two, perm_2_var_idxes);
+		if (use_mask) {
+			mask_64_1 = (len >= 32) ? UINT64_MAX : UINT64_MAX << (64 - 2*len);
+			mask_64_2 = (len >= 32) ? UINT64_MAX << (64 - 2*len) : 0;
+	                c = _mm512_maskz_permutex2var_epi8(mask_64_1, a, perm_2_var_idxes, b);
+        	        d = _mm512_maskz_permutex2var_epi8(mask_64_2, a, tmp, b);
+		} else {
+	                c = _mm512_permutex2var_epi8(a, perm_2_var_idxes, b);
+        	        d = _mm512_permutex2var_epi8(a, tmp, b);
+		}
+
+                *lens_one = _mm512_maskz_permutexvar_epi8(0x1111111111111111, permute_lens, lens_32);
+                tmp = _mm512_maskz_add_epi8(0x1111111111111111, permute_lens, thirty_two);
+                *lens_two = _mm512_maskz_permutexvar_epi8(0x1111111111111111, tmp, lens_32);
+                /* Update c and d by merging two 16bit codes */
+                const __m512i shifts_1 = _mm512_set_epi16(0, 0, 2, 0, 4, 0, 6, 0,
+                                8, 0, 10, 0, 12, 0, 14, 0,
+                                16, 0, 18, 0, 20, 0, 22, 0,
+                                24, 0, 26, 0, 28, 0, 30, 0);
+                tmp = _mm512_maskz_mov_epi8(0x5555555555555555, thirty_two);
+                __m512i shifts_2 = _mm512_maskz_add_epi16(0xaaaaaaaa, shifts_1, tmp);
+                __m512i tmp1, tmp2;
+
+                tmp1 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, shifts_1, lens_64);
+                tmp2 = _mm512_maskz_permutexvar_epi8(0x4444444444444444, shifts_2, lens_64);
+
+                shifts = _mm512_maskz_sub_epi8(0x4444444444444444, sixteen, tmp1);
+                shifts_back = _mm512_shuffle_epi8(shifts, perm_2b);
+                c = _mm512_srlv_epi16(c, shifts);
+                c = _mm512_sllv_epi32(c, shifts_back);
+
+                shifts = _mm512_maskz_sub_epi8(0x4444444444444444, sixteen, tmp2);
+                shifts_back = _mm512_shuffle_epi8(shifts, perm_2b);
+                d = _mm512_srlv_epi16(d, shifts);
+                d = _mm512_sllv_epi32(d, shifts_back);
+
+                *codes_one = c;
+                *codes_two = d;
+
+                return 2;
+        }
+
+        return 0;
+}
+
+
+
+
+static int
+huff_encode_avx512(uint8_t *dst, const uint8_t *src, int src_len)
+{
+	__m512i	bytes, lens_64, lens, codes;
+	__m512i rem_code = (__m512i)_mm512_setzero();
+	__m512i rem_len = (__m512i)_mm512_setzero();
+	__m512i lens_1, lens_2, lens_3, lens_4;
+	__m512i tmp;
+	__m512i seq = _mm512_set_epi32(0, 1, 2, 3,
+					4, 5, 6, 7,
+					8, 9, 10, 11,
+					12, 13, 14, 15);
+	const __m512i sixteen = _mm512_set1_epi32(16);
+	__mmask64 k, l;
+	int total_len = 0;
+	uint8_t chunk_len = 0;
+	uint8_t rem_len_scalar = 0;
+	int i;
+        int ret;
+        __m512i merged_codes_1, merged_codes_2, merged_lens_1, merged_lens_2;
+
+	lens_1 = _mm512_load_epi64(code_len);
+	lens_2 = _mm512_load_epi64(code_len + 64);
+	lens_3 = _mm512_load_epi64(code_len + 128);
+	lens_4 = _mm512_load_epi64(code_len + 192);
+
+	while (src_len >= 64) {
+		bytes = _mm512_loadu_si512(src);
+		src += 64;
+		src_len -= 64;
+		/* get corresponding code lengths */
+		tmp = _mm512_permutex2var_epi8(lens_1, bytes, lens_2);
+		lens = _mm512_permutex2var_epi8(lens_3, bytes, lens_4);
+		k = _mm512_movepi8_mask(bytes);
+		lens_64 = _mm512_mask_blend_epi8(k, tmp, lens);
+
+		ret = merge_shorter_codes(bytes, lens_64, 0, &merged_codes_1,
+			&merged_codes_2, &merged_lens_1, &merged_lens_2, 0);
+
+                if (ret == 1) {
+                        rem_code = compress_codes(merged_codes_1, merged_lens_1, rem_code,
+                                &rem_len, &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        continue;
+                } else if (ret == 2) {
+                        rem_code = compress_codes(merged_codes_1, merged_lens_1, rem_code,
+                                &rem_len, &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        rem_code = compress_codes(merged_codes_2, merged_lens_2, rem_code,
+                                &rem_len, &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        continue;
+                } else {
+#ifdef USE_LOOP
+			for (i = 0; i < 4; i++) {
+				/*
+				 * permutexvar uses only 6 LSB of idx so no worry
+				 * about overflow caused by _mm512_add_epi32
+				 */
+				tmp = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+				lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+				seq = _mm512_add_epi8(seq, sixteen);
+				codes = _mm512_i32gather_epi32(tmp, codes_tbl, 4);
+
+				rem_code = compress_codes(codes, lens, rem_code,
+					&rem_len, &rem_len_scalar, &chunk_len, dst);
+				dst += chunk_len;
+				total_len += chunk_len;
+			}
+#else
+                        __m512i a, b, c, d;
+                        load_codes(bytes, &a, &b, &c, &d);
+
+                        lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+                        seq = _mm512_add_epi8(seq, sixteen);
+                        rem_code = compress_codes(a, lens, rem_code, &rem_len,
+                                &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+                        seq = _mm512_add_epi8(seq, sixteen);
+                        rem_code = compress_codes(b, lens, rem_code, &rem_len,
+                                &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+                        seq = _mm512_add_epi8(seq, sixteen);
+                        rem_code = compress_codes(c, lens, rem_code, &rem_len,
+                                &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+
+                        lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+                        seq = _mm512_add_epi8(seq, sixteen);
+                        rem_code = compress_codes(d, lens, rem_code, &rem_len,
+                                &rem_len_scalar, &chunk_len, dst);
+                        dst += chunk_len;
+                        total_len += chunk_len;
+#endif
+		}
+	}
+
+#define NGHTTP_HUFF_AVX512_THRESH	12
+	if (src_len > NGHTTP_HUFF_AVX512_THRESH) {
+		const __m512i zero = (__m512i)_mm512_setzero();
+		l = (1ULL << src_len) - 1;
+		bytes = _mm512_maskz_loadu_epi8(l, src);
+		tmp = _mm512_maskz_permutex2var_epi8(l, lens_1, bytes, lens_2);
+		lens = _mm512_maskz_permutex2var_epi8(l, lens_3, bytes, lens_4);
+		k = _mm512_movepi8_mask(bytes);
+		lens_64 = _mm512_mask_blend_epi8(k, tmp, lens);
+
+		ret = merge_shorter_codes(bytes, lens_64, src_len, &merged_codes_1,
+			&merged_codes_2, &merged_lens_1, &merged_lens_2, 1);
+
+		if (ret == 1) {
+        		rem_code = compress_codes(merged_codes_1, merged_lens_1, rem_code,
+				&rem_len, &rem_len_scalar, &chunk_len, dst);
+	        dst += chunk_len;
+		total_len += chunk_len;
+
+		} else if (ret == 2) {
+			rem_code = compress_codes(merged_codes_1, merged_lens_1, rem_code,
+				&rem_len, &rem_len_scalar, &chunk_len, dst);
+			dst += chunk_len;
+			total_len += chunk_len;
+
+			rem_code = compress_codes(merged_codes_2, merged_lens_2, rem_code,
+				&rem_len, &rem_len_scalar, &chunk_len, dst);
+			dst += chunk_len;
+			total_len += chunk_len;
+		} else {
+			while (src_len > 0) {
+				uint8_t part_len = (src_len > 16) ? 16 : src_len;
+				tmp = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, bytes);
+				lens = _mm512_maskz_permutexvar_epi8(0x1111111111111111, seq, lens_64);
+				seq = _mm512_add_epi8(seq, sixteen);
+				__mmask16 k_16 = ~((1 << (16 - part_len)) - 1);
+				codes = _mm512_mask_i32gather_epi32(zero, k_16, tmp, codes_tbl, 4);
+
+				rem_code = compress_codes(codes, lens, rem_code,
+					&rem_len, &rem_len_scalar, &chunk_len, dst);
+				dst += chunk_len;
+				total_len += chunk_len;
+				src_len -= 16;
+			}
+		}
+	} else {
+		uint64_t acc = 0;
+		uint64_t cur_code;
+		uint8_t pending_len = 0;
+		uint8_t cur_len;
+		uint8_t byte;
+		if (rem_len_scalar != 0) {
+			const __m512i rem_bits = _mm512_set1_epi64(0x3f3e3d3c3b3a3938);
+			acc = _mm512_bitshuffle_epi64_mask(rem_code, rem_bits);
+			pending_len = rem_len_scalar;
+		}
+
+		for (i = 0; i < src_len; i++) {
+			byte = src[i];
+			cur_code = codes_tbl[byte];
+			cur_len = code_len[byte];
+			cur_code <<= 32;
+			acc |= cur_code >> pending_len;
+			pending_len += cur_len;
+			if (pending_len < 64)
+				continue;
+
+			*(uint64_t *)(dst) = __builtin_bswap64(acc);
+			dst += 8;
+			total_len += 8;
+			pending_len -= 64;
+			acc = pending_len ? cur_code << (cur_len - pending_len) : 0;
+		}
+		if (pending_len & 0x7) {
+			acc |= UINT64_MAX >> pending_len;
+			pending_len &= ~0x7;
+			pending_len += 8;
+		}
+
+		for (i = 0; i < (pending_len >> 3); i++) {
+			*dst = (uint8_t)(acc >> (56 - 8*i));
+			dst++;
+			total_len++;
+		}
+	}
+
+	if (rem_len_scalar != 0) {
+		const __m512i rem_bits = _mm512_set1_epi64(0x3f3e3d3c3b3a3938);
+		l = _mm512_bitshuffle_epi64_mask(rem_code, rem_bits);
+		uint8_t last_byte = l >> 56;
+		last_byte |= (1 << (8 - rem_len_scalar)) - 1;
+		*dst = last_byte;
+		total_len++;
+	}
+
+	/* add final scalar handling */
+	return total_len;
+}
+
+#endif
--
2.25.1

